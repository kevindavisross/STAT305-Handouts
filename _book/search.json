[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 305 Handouts",
    "section": "",
    "text": "Preface\nA selection of Handouts for STAT 305.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "randomness-and-probability.html",
    "href": "randomness-and-probability.html",
    "title": "1  Randomness and Probability",
    "section": "",
    "text": "Probability comes up in a wide variety of situations. Consider just a few examples.\n\nThe probability that you roll doubles in a turn of a board game.\nThe probability you win the next Powerball lottery if you purchase a single ticket, 4-8-15-16-42, plus the Powerball number, 23.\nThe probability that a “randomly selected” Cal Poly student is a California resident.\nThe probability that the high temperature in San Luis Obispo next Tuesday is above 90 degrees F.\nThe probability that the Philadelphia Eagles win the next Superbowl.\nThe probability that the Republican candidate wins the 2032 U.S. Presidential Election.\nThe probability that extraterrestrial life currently exists somewhere in the universe.\nThe probability that you ate an apple on April 17, 2009.\n\n\nExample 1.1 How are the situations above similar, and how are they different? What is one feature that all of the situations have in common? Is the interpretation of “probability” the same in all situations? The goal here is to just think about these questions, and not to compute any probabilities (or to even think about how you would).\n\n\n\n\n\n\n\n\nA phenomenon is random if there are multiple potential outcomes, and there is uncertainty about which outcome will occur.\nUncertainty is understood in broad terms, and in particular does not only concern future occurrences.\nMany phenomena involve physical randomness, like flipping a coin or drawing powerballs at random from a bin, or in statistical applications of random sampling or random assignment.\nBut in many other situations, randomness just vaguely reflects uncertainty.\nRandom does not mean haphazard. In a random phenomenon, while individual outcomes are uncertain, there is a regular distribution of outcomes over a large number of (hypothetical) repetitions.\nAlso, random does not necessarily mean equally likely. In a random phenomenon, certain outcomes or events might be more or less likely than others.\nThe probability of an event associated with a random phenomenon is a number in the interval \\([0, 1]\\) measuring the event’s likelihood or degree of uncertainty. A probability can take any value in the continuous scale from 0% to 100%.\nThere are two main interpretations of probability.\n\nLong run relative frequency. The probability of an event can be interpreted as the proportion of times that the event would occur in a very large number of hypothetical repetitions of the random phenomenon.\nSubjective probability. There are many situations where the outcome is uncertain, but it does not make sense to consider the situation as repeatable. In such situations, a subjective (a.k.a., personal) probability describes the degree of likelihood a given individual ascribes to a certain event. Think of subjective probabilities as measuring relative degrees of likelihood rather than long run relative frequencies.\n\nFortunately, the mathematics of probability work the same way regardless of the interpretation. In either case, the same basic logical consistency requirements must be satisfied.\nA simulation involves an artificial recreation of the random phenomenon, usually using a computer. The probability of an event can be approximated by simulating the random phenomenon a large number of times and determining the proportion of simulated repetitions on which the event occurred out of the total number of repetitions in the simulation.\n\n\nExample 1.2 One of the oldest documented problems in probability is the following: If three fair six-sided dice are rolled, what is more likely: a sum of 9 or a sum of 10?\n\n\nExplain how you could conduct a simulation to investigate this question.\n\n\n\n\n\nIn 1 million repetitions of a simulation, a sum of 9 occurred in 115392 repetitions and a sum of 10 occurred in 125026 repetitions. Use the simulation results to approximate the probability that the sum is 9; repeat for a sum of 10.\n\n\n\n\n\nIt can be shown that the theoretical probability that the sum is 9 is 25/216 = 0.116. Write a clearly worded sentence interpreting this probability as a long run relative frequency.\n\n\n\n\n\nIt can be shown that the theoretical probability that the sum is 10 is 27/216 = 0.125. How many times more likely is a sum of 10 than a sum of 9?\n\n\n\n\n\n\n\nExample 1.3 The weather forecast calls for a 30% chance of rain in your city tomorrow. You ask Donny Don’t to interpret the 30% as a long run relative frequency. Donny says: “it will rain in 30% of the city tomorrow”. You ask him to elaborate; he says: “Well, there are many different locations in the city. In some of the locations it will rain, in some it won’t. It will rain in 30% of the locations, and not in the other 70%. That is, rain will cover 30% of the area of the city, and the other 70% won’t have rain.” Do you agree? If not, how would you interpret the 30% as a long run relative frequency?\n\n\n\n\n\n\n\n\nWhen interpreting the long run, be careful to define the random phenomenon that is being repeated\n\n\nExample 1.4 In the first 7 games of his NBA career, Paolo Banchero attempted 60 free throws and successfully made 44. Donny Don’t says “the probability that Paolo Banchero successfully makes a free throw attempt is 44/60 = 0.733.” Do you agree? Explain.\n\n\n\n\n\n\n\n\nDistinguish between the short run and the long run\nObserved relative frequencies based on past data (sometimes called “empirical probabilities”) are only short run approximations to theoretical probabilities which represent long run relative frequencies\n\n\nExample 1.5 Your favorite local weatherperson forecasts a 30% chance of rain tomorrow and a 60% chance of rain the next day in your city.\n\nExplain how these probabilities are subjective.\n\n\n\n\n\nYou ask Donny Don’t to interpret these values as relative degrees of likelihood. Donny says: “Well, 30% is not that big, so it’s not going to rain that hard tomorrow. Also, 60% is twice is big as 30%, so it’s going to rain twice as hard two days from now as it will tomorrow”. Do you agree? Explain.\n\n\n\n\n\nDonny says: “Can’t we just look at the data from all the days with weather conditions similar to the ones forecast for tomorrow, and see how often it rained on those days to find the probability of rain tomorrow? No subjectivity about that!” How would you respond?\n\n\n\n\n\n\n\n\n\nA probabilistic forecast combines observed data and statistical or mathematical models to make predictions.\nRather than providing a single prediction such as “it will rain tomorrow”, probabilistic forecasts provide a range of scenarios and their relative likelihoods.\nSuch forecasts are subjective in nature, relying upon the data used and assumptions of the model.\nChanging the data or assumptions can result in different forecasts and probabilities.\n\n\nExample 1.6 What is your subjective probability that Professor Ross (the author) has a TikTok account? Consider the following two bets, and suppose you must choose only one.\n\nYou win $100 if Professor Ross has a TikTok account, and you win nothing otherwise.\nA box contains 40 green and 60 gold marbles that are otherwise identical. The marbles are thoroughly mixed and one marble is selected at random. You win $100 if the selected marble is green, and you win nothing otherwise.\n\n\nWhich of the above bets would you prefer? Or are you completely indifferent? What does this say about your subjective probability that Professor Ross has a Tik Tok account?\nIf you preferred bet B to bet A, consider bet C which has a similar setup to B but now there are 20 green and 80 gold marbles. Do you prefer bet A or bet C? What does this say about your subjective probability that Professor Ross has a Tik Tok account?\nIf you preferred bet A to bet B, consider bet D which has a similar setup to B but now there are 60 green and 40 gold marbles. Do you prefer bet A or bet D? What does this say about your subjective probability that Professor Ross has a Tik Tok account?\nContinue to consider different numbers of green and gold marbles. Can you zero in on your subjective probability?\n\n\n\n\n\n\n\n\nExample 1.7 As of Jun 13, FanGraphs listed the following probabilities for who will win the 2025 MLB World Series.\n\n\n\n\nTeam\nProbability\n\n\n\n\nDodgers\n21%\n\n\nYankees\n17%\n\n\nTigers\n10%\n\n\nMets\n10%\n\n\nPhillies\n7%\n\n\nOther\n\n\n\n\nAccording to FanGraphs (as of Jun 13):\n\nAre the above percentages relative frequencies or subjective probabilities? Why?\n\n\nWhat must be the probability that a team other than the above five teams wins the championship? That is, what value goes in the “Other” row in the table?\n\n\n\n\n\nThe Dodgers are how many times more likely than the Phillies to win?\n\n\n\n\n\nWhat must be the probability that the Dodgers do not win the championship? How many times more likely are the Dodgers to not win than to win (this ratio is the “odds against” the Dodgers winning).\n\n\n\n\n\nHow could you construct a circular spinner (like from a kids game) to simulate the World Series champion according to these probabilities? According to this model, what would you expect the results of 10000 repetitions of a simulation of the champion to look like?\n\n\n\n\n\n\n\nExample 1.8 Suppose your subjective probabilities for the 2025 World Series champion satisfy the following conditions.\n\nThe Brewers and Yankees are equally likely to win\nThe Phillies are 1.5 times more likely than the Yankees to win\nThe Dodgers are 2 times more likely than the Phillies to win\nThe winner is as likely to be among these four teams — Brewers, Yankees, Phillies, Dodgers — as not.\n\nConstruct a table of your subjective probabilities like the one in Example 1.7.\n\n\n\n\n\n\n\n\nThe previous examples illustrate two interpretations of probability: long run relative frequencies and subjective probabilities.\nWe will use these interpretations interchangeably.\nWith subjective probabilities it is often helpful to consider what might happen in a simulation.\nIt is also useful to consider long run relative frequencies in terms of relative degrees of likelihood.\nFortunately, the mathematics of probability work the same way regardless of the interpretation.\nA probability takes a value in the sliding scale from 0 to 1 (or 0 to 100%).\nDon’t just focus on computation; always remember to properly interpret probabilities.\n\n\nExample 1.9 Consider a Cal Poly student who frequently has blurry, bloodshot eyes, generally exhibits slow reaction time, always seems to have the munchies, and disappears at 4:20 each day. Which of the following, A or B, has a higher probability? Assume the two probabilities are not equal.\n\nA: The student has a GPA above 3.0.\nB: The student has a GPA above 3.0 and smokes marijuana regularly.\n\n\n\n\n\n\n\n\n\nWarning! Your psychological judgment of probabilities is often inconsistent with the mathematical logic of probabilities.\n\n\nExample 1.10 Ron and Leslie agree to the following bet. They’ll ask Professor Ross if he has a TikTok account. If he does, Leslie will pay Ron $200; if not, Ron will pay Leslie $100. (Neither has any direct information about whether or not Professor Ross has a TikTok account.)\n\nGiven this setup, which of the following is being judged as more likely: that Professor Ross has a TikTok account, or that he does not? Why?\n\n\n\n\n\nWhat are this bet’s “odds”?\n\n\n\n\n\nRon and Leslie agree that this is a fair bet, and neither would accept worse odds. What is their subjective probability that Professor Ross has a TikTok account?\n\n\n\n\n\nSuppose they were to hypothetically repeat this bet many times, say 3000 times. Given the probability from the previous part, how many times would you expect Leslie to win? To lose? What would you expect Leslie’s net dollar winnings to be? In what sense is this bet “fair”? (Remember: Leslie’s winnings are Ron’s losses and vice versa.)\n\n\n \n\n\n\n\nThe odds of an event is a ratio involving the probability that the event occurs and the probability that the event does not occur.\nOdds can be expressed as either “in favor” of or “against” the event occurring, depending on the order of the ratio.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Randomness and Probability</span>"
    ]
  },
  {
    "objectID": "working-with-probabilities.html#footnotes",
    "href": "working-with-probabilities.html#footnotes",
    "title": "2  Working with Probabilities",
    "section": "",
    "text": "The values in this problem are based on a March 12, 2025 article by the Pew Research Center.↩︎\nThese values are based on the 2018 General Social Survey.↩︎\nSource: http://www.cdc.gov/ncbddd/birthdefects/downsyndrome/data.html↩︎\nEstimates of these probabilities vary between different sources. The values in the exercise were based on https://www.ncbi.nlm.nih.gov/pubmed/17350315↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Working with Probabilities</span>"
    ]
  },
  {
    "objectID": "interpreting-probabilities.html",
    "href": "interpreting-probabilities.html",
    "title": "3  Interpreting Probabilities and “Expected” Values",
    "section": "",
    "text": "A probability takes a value in the sliding scale from 0 to 100%.\nDon’t just focus on computation; always remember to properly interpret probabilities.\n\n\nExample 3.1 In each of the following parts, which of the two probabilities, a or b, is larger, or are they equal? You should answer conceptually without attempting any calculations. Explain your reasoning.\n\n\nRandomly select a man.\n\nThe probability that a randomly selected man who is greater than six feet tall plays in the NBA.\nThe probability that a randomly selected man who plays in the NBA is greater than six feet tall.\n\nRandomly select a baby girl who was born in 1950.\n\nThe probability that a randomly selected baby girl born in 1950 is alive today.\nThe probability that a randomly selected baby girl born in 1950, who was alive at the end of 2020, is alive today.          \n\n\n\nA probability is a measure of the likelihood or degree of uncertainty or plausibility of an event.\nA “conditional” probability revises this measure to reflect any additional information about the outcome of the underlying random phenomenon.\nIn a sense, all probabilities are conditional upon some information, even if that information is vague (“well, it has to be one of these possibilities”). Be careful to clearly identify what information is reflected in probabilities\nWhen interpreting probabilities, consider the conditions under which the probabilities were computed, in the proper direction\n\n\nExample 3.2 In each of the following parts, which of the two probabilities, a or b, is larger, or are they equal? You should answer conceptually without attempting any calculations. Explain your reasoning.\n\n\nFlip a coin which is known to be fair 10 times.\n\nThe probability that the results are, in order, HHHHHHHHHH.\nThe probability that the results are, in order, HHTHTTTHHT.\n\nFlip a coin which is known to be fair 10 times.\n\nThe probability that all 10 flips land on H.\nThe probability that exactly 5 flips land on H.\n\nIn the Powerball lottery there are roughly 300 million possible winning number combinations, all equally likely.\n\nThe probability you win the next Powerball lottery if you purchase a single ticket, 4-8-15-16-42, plus the Powerball number, 23\nThe probability you win the next Powerball lottery if you purchase a single ticket, 1-2-3-4-5, plus the Powerball number, 6.\n\nContinuing with the Powerball\n\nThe probability that the numbers in the winning number are not in sequence (e.g., 4-8-15-16-42-23)\nThe probability that the numbers in the winning number are in sequence (e.g., 1-2-3-4-5-6)\n\nContinuing with the Powerball\n\nThe probability that you win the next Powerball lottery if you purchase a single ticket.\nThe probability that someone wins the next Powerball lottery. (FYI: especially when the jackpot is large, there are hundreds of millions of tickets sold.)          \n\n\n\nWhen interpreting probabilities, be careful not to confuse “the particular” with “the general”.\n\n“The particular:” A very specific event, surprising or not, often has low probability.\n“The general:” While a very specific event often has low probability, if there are many like events their combined probability can be high.\n\nEven if an event has extremely small probability, given enough repetitions of the random phenomenon, the probability that the event occurs on at least one of the repetitions is often high.\nIn general, even though the probability that something very specific happens to you today is often extremely small, the probability that something similar happens to someone some time is often quite high.\nWhen assessing a numerical probability, always ask “probability of what”? Does the probability represent “the particular” or “the general”? Is it the probability that the event happens in a single occurrence of the random phenomenon, or the probability that the event happens at least once in many occurrences?\nAlso distinguish between assumption and observation. For example, if you assume that a coin is fair and the flips are independent, then all possible H/T sequences are equally likely. However, if you observe the coin landing on heads on 20 flips in a row, then that might cast doubt on your assumption that the coin is fair.\n\n\nExample 3.3 Shuffle a standard deck of 52 playing cards (13 face values in each of 4 suits) and deal two cards without replacement.\n\n\nWhat is the probability that the first card dealt is a heart?\n\n\n\n\n\nWhat is the probability that the second card dealt is a heart?\n\n\n\n\n\nWhat is the probability that the second card dealt is a heart if the first card dealt is a heart?\n\n\n\n\n\nWhat is the probability that the second card dealt is a heart if the first card dealt is not a heart?\n\n\n\n\n\nRevisit part 2. What is the probability that the second card dealt is a heart? Create a two-way table to answer this question.\n\n\n\n\n\n\n\nBe careful to distinguish between conditional and unconditional probabilities.\nA conditional probability reflects additional information about the outcome of the random phenomenon.\nIn the absence of such information, we must continue to account for all the possibilities.\nWhen computing probabilities, be sure to only reflect information that is known. Especially when considering a phenomenon that happens in stages, don’t assume that when considering what happens second that you know what happened first.\n\n\nExample 3.4 Within both the colleges of Agriculture and Architecture at Cal Poly, about 49% of admitted students are female, about 84% of admitted students went to high school in CA, and the median GPA of admitted students is about 4.1.\nAn orientation group of 100 newly admitted Cal Poly students includes 75 students in Agriculture and 25 students in Architecture. A student is randomly selected from this group. The selected student is Maddie, who is female, went to high school in CA, and had a high school GPA of 4.1.\n\nDonny Don’t says, “The information about Maddie applies equally well to Agriculture or Architecture and doesn’t help us decide which college she’s in, so it’s just 50/50. Given the information about Maddie, the conditional probability that she is in Agriculture is 0.5.” Do you agree? If not, what is the conditional probability that Maddie is in the college of Agriculture given the information about her?\n\n\n\n\n\n\nExample 3.5 This is a very simplified example illustrating the basic idea of how insurance works. Every year an insurance company sells many thousands of car insurance policies to drivers within a particular risk class. Each policyholder pays a “premium” of $1000 at the start of the year, and the insurance company agrees to pay for the cost of all damages that occur during the year. Suppose that each policy incurs damage of either $0, $5000, $20000, or $50000 with the following probabilities.\n\n\n\nAmount of damage ($)\nProfit ($)\nProbability\n\n\n\n\n0\n1000\n0.910\n\n\n5000\n-4000\n0.070\n\n\n20000\n-19000\n0.019\n\n\n50000\n-49000\n0.001\n\n\n\nThe insurance company’s profit on a policy at the end of the year is the difference between the premium of $1000 and any damage paid out. For example, a policy that incurs no damage results in a profit of $1000; a policy that incurs $5000 in damage results in a profit of -$4000 (that is, a loss of $4000) for the insurance company.\n\n\nInterpret the probabilities 0.91, 0.07, 0.019, and 0.001 as long run relative frequencies in this context.\n\n\n\n\n\nCompute the probability that a policy results in a positive profit for the insurance company.\n\n\n\n\n\nImagine 100,000 hypothetical policies. How many of these policies would you expect to result in a profit of $1000? -$4000? -$19000? -$49000?\n\n\n\n\n\nWhat do you expect the total profit for these 100,000 policies to be?\n\n\n\n\n\nWhat do you expect the average profit per policy for these 100,000 policies to be?\n\n\n\n\n\nCompute the probability that a policy has a profit equal to the value from part 5.\n\n\n\n\n\nCompute the probability that a policy has a profit greater than the value from part 5.\n\n\n\n\n\nIs the value from part 5 the most likely value of profit for a single policy?\n\n\n\n\n\nIs the value from part 5 the profit you would expect for a single policy?\n\n\n\n\n\nExplain in what sense the value from part 5 is “expected”.\n\n\n\n\n\n\n\nThe long run average value of a random quantity is called its “expected value”.\nBe careful: the term “expected value” is somewhat of a misnomer.\nThe expected value is not necessarily the value we expect on a single repetition of the random phenomenon, nor the most likely value (or even a possible value).\nRather, the expected is the value we expect to see on average in the long run over many repetitions.\nA probability can be interpreted as a long run relative frequency; an expected value can be interpreted as a long run average value.\n\n\nExample 3.6 Continuing Example 3.5. We considered what we would expect for 100000 hypothetical policies, but what about an unspecified large number of policies?\n\n\nImagine that we have recorded the profit for each of a large number of policies (not necessarily 100000). Explain in words the process by which you would compute the average profit per policy. (In other, more general, words: how do you compute an average of a list of numbers?)\n\n\n\n\n\nGiven that the profit of any policy is either 1000, -4000, -19000, or -49000, how could we simplify the calculation of the sum in the previous part? Write a general expression for the average profit per policy in this scenario.\n\n\n\n\n\nWhat do you think the expression in the previous part converges to in the long run?\n\n\n\n\n\nExplain how the value in the previous part is a “probability-weighted average value”.\n\n\n\n\n\nCompute the expected value of damage (not profit) as a probability-weighted average value.\n\n\n\n\n\nInterpret the value from the previous part as a long run average value in this context.\n\n\n\n\n\nHow is the expected value of profit related to the expected value of damage? Does this make sense? Why?\n\n\n\n\n\n\n\nAn expected value can be computed as a “probability-weighted average value”\nBut this is just a more compact way of computing an average in the usual way: add up all the values and divide by the number of values.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Interpreting Probabilities and \"Expected\" Values</span>"
    ]
  },
  {
    "objectID": "outcomes-events-rvs.html",
    "href": "outcomes-events-rvs.html",
    "title": "4  Outcomes, Events, and Random Variables",
    "section": "",
    "text": "4.1 Outcomes\nProbability models can be applied to any situation in which there are multiple potential outcomes and there is uncertainty about which outcome will occur.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Outcomes, Events, and Random Variables</span>"
    ]
  },
  {
    "objectID": "outcomes-events-rvs.html#outcomes",
    "href": "outcomes-events-rvs.html#outcomes",
    "title": "4  Outcomes, Events, and Random Variables",
    "section": "",
    "text": "Due to the wide variety of types of random phenomena, an outcome can be virtually anything\nIn particular, an outcome does not have to be a number.\nThe sample space, denoted \\(\\Omega\\) (the uppercase Greek letter “omega”), is the set of all possible outcomes of a random phenomenon. An outcome, denoted \\(\\omega\\) (the lowercase Greek letter “omega”), is an element of the sample space: \\(\\omega\\in\\Omega\\).\nMathematically, the sample space \\(\\Omega\\) is a set containing all possible outcomes, while an individual outcome \\(\\omega\\) is a point or element in \\(\\Omega\\).\nA random phenomenon is modeled by a single sample space, with respect to which all objects (events, random variables) are defined. Whenever possible, a sample space outcome should be defined to provide the maximum amount of information about the outcome of random phenomenon.\nIn practice we rarely enumerate the sample space as we’ll for some of the examples in this class. Nonetheless, there is always some underlying sample space corresponding to all possible outcomes of the random phenomenon.\n\n\nExample 4.1 Roll a four-sided die twice, and record the result of each roll in sequence as an ordered pair. For example, the outcome \\((3, 1)\\) represents a 3 on the first roll and a 1 on the second; this is not the same outcome as \\((1, 3)\\).\n\n\nHow many possible outcomes are there? Identify the sample space.\n\n\n\n\n\nWe might be interested in the sum of the two dice. Explain why it is still advantageous to define the sample space as in the previous part, rather than as \\(\\Omega=\\{2, \\ldots, 8\\}\\).\n\n\n\n\n\n\n\nMultiplication principle for counting Suppose that stage 1 of a process can be completed in any one of \\(n_1\\) ways. Further, suppose that for each way of completing the stage 1, stage 2 can be completed in any one of \\(n_2\\) ways. Then the two-stage process can be completed in any one of \\(n_1\\times n_2\\) ways.\nThis rule extends naturally to a \\(\\ell\\)-stage process, which can then be completed in any one of \\(n_1\\times n_2\\times n_3\\times\\cdots\\times n_\\ell\\) ways.\nIt is not important whether there is a “first” or “second” stage. What is important is that there are distinct stages, each with its own number of “choices”.\n\n\nExample 4.2 The “matching problem” is one well known probability problem. The general setup involves \\(n\\) distincts objects labeled \\(1, \\ldots, n\\) which are placed in \\(n\\) distinct boxes labeled \\(1, \\ldots, n\\), with exactly one object placed in each box.\n\nConsider the matching problem with \\(n=3\\). Label the objects 1, 2, 3, and the spots 1, 2, 3, with spot 1 the correct spot for object 1, etc. Specify an appropriate definition of an outcome, determine the number of outcomes, and specify the sample space.\n\n\n\n\n\nFor a general \\(n\\), how many possible outcomes are there?\n\n\n\n\n\n\n\n\nExample 4.3 Regina and Cady plan to meet for lunch They will definitely arrive between noon and 1, but their exact arrival times are uncertain. Rather than dealing with clock time, it is helpful to represent noon as time 0 and measure time as minutes after noon, including fractions of a minute, so that arrival times take values in the continuous interval [0, 60].\nSpecify an appropriate definition of an outcome and draw a picture representing the sample space.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Outcomes, Events, and Random Variables</span>"
    ]
  },
  {
    "objectID": "outcomes-events-rvs.html#events",
    "href": "outcomes-events-rvs.html#events",
    "title": "4  Outcomes, Events, and Random Variables",
    "section": "4.2 Events",
    "text": "4.2 Events\n\nAn event is something that could happen or might be true.\nAn event is a collection of outcomes that satisfy some criteria.\nMathematically, an event \\(A\\) is a subset of the sample space: \\(A\\subseteq \\Omega\\).\nEvents are typically denoted with capital letters near the start of the alphabet, with or without subscripts (e.g. \\(A\\), \\(B\\), \\(C\\), \\(A_1\\), \\(A_2\\)). Events can be composed from others using basic set operations like unions (\\(A\\cup B\\)), intersections (\\(A \\cap B\\)), and complements (\\(A^c\\)).\n\nRead \\(A^c\\) as “not \\(A\\)”.\nRead \\(A\\cap B\\) as “\\(A\\) and \\(B\\)”\nRead \\(A \\cup B\\) as “\\(A\\) or \\(B\\)”. Note that unions (\\(\\cup\\), “or”) are always inclusive. \\(A\\cup B\\) occurs if \\(A\\) occurs but \\(B\\) does not, \\(B\\) occurs but \\(A\\) does not, or both \\(A\\) and \\(B\\) occur.\n\nA collection of events \\(A_1, A_2, \\ldots\\) are disjoint (a.k.a. mutually exclusive) if \\(A_i \\cap A_j = \\emptyset\\) for all \\(i \\neq j\\). That is, multiple events are disjoint if none of the events have any outcomes in common.\nIf the sample space outcomes are represented by rows in a spreadsheet, then an event is a subset of rows that satisfies some criteria\n\n\nExample 4.4 Matching problem. For \\(n=3\\), objects labeled 1, 2, 3, are placed at random in spots labeled 1, 2, 3, with spot 1 the correct spot for object 1, etc. Using the sample space from Example 4.2, identify the following events.\n\n\n\\(B\\), the event that no objects are put in the correct spot.\n\n\n\n\n\nIn words, what does \\(B^c\\) represent?\n\n\n\n\n\n\\(A\\), the event that all objects are put in the correct spot.\n\n\n\n\n\n\\(C\\), the event that exactly 2 objects are put in the correct spot.\n\n\n\n\n\n\\(A_3\\), the event that object 3 is put (correctly) in spot 3.\n\n\n\n\n\nFor a general \\(n\\) let \\(A\\) the event that all objects are put in the correct spot, let \\(B\\) the event that no objects are put in the correct spot, and let \\(A_i\\) be the event that object \\(i\\) is put (correctly) in spot \\(i\\), \\(i=1,\\ldots, n\\). What is the relationship between \\(A\\) and \\(A_1, \\ldots, A_n\\)? What is the relationship between \\(B\\) and \\(A_1, \\ldots, A_n\\)?\n\n\n\n\n\n\n\nExample 4.5 Using the sample space from Example 4.3, identify the following events using pictures.\n\n\nIdentify \\(A\\), the event that Regina arrives after Cady.\n\n\n\n\n\nIdentify \\(B\\), the event that either Regina or Cady arrives before 12:30.\n\n\n\n\n\nIdentify \\(C\\), the event that they arrive within 15 minutes of each other.\n\n\n\n\n\nIdentify \\(D\\), the event that Regina arrives before 12:24.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Outcomes, Events, and Random Variables</span>"
    ]
  },
  {
    "objectID": "outcomes-events-rvs.html#random-variables",
    "href": "outcomes-events-rvs.html#random-variables",
    "title": "4  Outcomes, Events, and Random Variables",
    "section": "4.3 Random variables",
    "text": "4.3 Random variables\n\nRoughly, a random variable assigns a number measuring some quantity of interest to each outcome of a random phenomenon.\nMathematically, a random variable (RV) \\(X\\) is a function that takes an outcome in the sample space as input and returns a real number as output\nThe random variable itself is typically denoted with a capital letter (\\(X\\)); possible values of that random variable are denoted with lower case letters (\\(x\\)).\n\nThink of the capital letter \\(X\\) as a label standing in for a formula like “the number of heads in 4 flips of a coin” and\n\\(x\\) as a dummy variable standing in for a particular value like 3.\n\nDiscrete random variables take at most countably many possible values (e.g., \\(0, 1, 2, \\ldots\\)). They are often counting variables (e.g., the number of Heads in 10 coin flips).\nContinuous random variables can take any real value in some interval (e.g., \\([0, 1]\\), \\([0,\\infty)\\), \\((-\\infty, \\infty)\\).). That is, continuous random variables can take uncountably many different values. Continuous random variables are often measurement variables (e.g., height, weight, income).\nA function of a random variable is also a random variable: if \\(X\\) is a RV then so is \\(g(X)\\)\nSums and products, etc., of random variables defined on the same sample space are random variables. If\\(X\\) and \\(Y\\) are RVs defined on the same sample space then so are \\(X+Y\\), \\(X-Y\\), \\(XY\\)\nIf the sample space outcomes are represented by rows in a spreadsheet, then random variables correspond to columns.\nExpressions like \\(X=x\\) or \\(\\{X=x\\}\\) represent events: for which outcomes is the value of the random variable \\(X\\) equal to the value \\(x\\)\n\n\nExample 4.6 Roll a four-sided die twice, and record the result of each roll in sequence. Recall the sample space from Example 4.1. Let \\(X\\) be the sum of the two dice, and let \\(Y\\) be the larger of the two rolls (or the common value if both rolls are the same).\n\n\n\n\nOutcome\n\\(X\\)\n\\(Y\\)\n\n\n\n\n(1, 1)\n\n\n\n\n(1, 2)\n\n\n\n\n(1, 3)\n\n\n\n\n(1, 4)\n\n\n\n\n(2, 1)\n\n\n\n\n(2, 2)\n\n\n\n\n(2, 3)\n\n\n\n\n(2, 4)\n\n\n\n\n(3, 1)\n\n\n\n\n(3, 2)\n\n\n\n\n(3, 3)\n\n\n\n\n(3, 4)\n\n\n\n\n(4, 1)\n\n\n\n\n(4, 2)\n\n\n\n\n(4, 3)\n\n\n\n\n(4, 4)\n\n\n\n\n\n\nConstruct a table identifying the values of \\(X\\) and \\(Y\\) for each outcome in the sample space.\n \nIdentify the possible values of \\(X\\).\n\n\n\n\n\nIdentify the possible values of \\(Y\\).\n\n\n\n\n\nIdentify the possible values of the pair \\((X, Y)\\).\n\n\n\n\n\nIdentify \\(\\{Y = 1\\}\\).\n\n\n\n\n\nIdentify \\(\\{Y = 2\\}\\).\n\n\n\n\n\nIdentify \\(\\{Y = 3\\}\\).\n\n\n\n\n\nIdentify \\(\\{Y = 4\\}\\).\n\n\n\n\n\nIdentify \\(\\{X \\le 4\\}\\).\n\n\n\n\n\nIdentify \\(\\{X = 4, Y = 3\\}\\)\n\n\n\n\n\n\n\nExample 4.7 Matching problem. For \\(n=3\\) objects labeled 1, 2, 3, are placed at random in spots labeled 1, 2, 3, with spot 1 the correct spot for object 1, etc. Recall the sample space from Example 4.2. Let the random variable \\(X\\) count the number of objects that are put in the correct spot. Let \\(I_1\\) be equal to 1 if object 1 is placed (correctly) in spot 1, and define \\(I_2, I_3\\) similarly.\n\n\n\n\nOutcome\n\\(X\\)\n\\(I_1\\)\n\\(I_2\\)\n\\(I_3\\)\n\n\n\n\n123\n\n\n\n\n\n\n132\n\n\n\n\n\n\n213\n\n\n\n\n\n\n231\n\n\n\n\n\n\n312\n\n\n\n\n\n\n321\n\n\n\n\n\n\n\n\nConstruct a table identifying the value of \\(X, I_1, I_2, I_3\\) for each outcome in the sample space.    \nIdentify the possible values of \\(X\\).\n\n\n\n\n\nWhat is the relationship between \\(I_3\\) and event \\(A_3\\) from Example 4.4?\n\n\n\n\n\nHow can you express \\(X\\) in terms of \\(I_1, I_2, I_3\\)?\n\n\n\n\n\n\n\nThe indicator (a.k.a., Bernoulli) random variable corresponding to event \\(A\\) is equal to 1 if \\(A\\) occurs and 0 otherwise\nIndicator random variables can be used for incremental counting and are often useful in problems involving “find the expected number of”\n\n\nExample 4.8 Regina and Cady will definitely arrive between noon and 1, but their exact arrival times are uncertain. Recall the sample space from Example 4.3. Let \\(R\\) be the random variable representing Regina’s arrival time (minutes after noon), and \\(Y\\) for Cady.\n\nWhat does the random variable \\(T = \\min(R, Y)\\) represent? What are the possible values of \\(T\\)?\n\n\n\n\n\nWhat does the random variable \\(W = |R - Y|\\) represent? What are the possible values of \\(W\\)?\n\n\n\n\n\nLet \\(N\\) be the number of people (out of 2) who arrive before 12:30. How can you represent \\(N\\) in terms of \\(R\\) and \\(Y\\). (Hint: use indicators.)\n\n\n\n\n\nIdentify each of the random variables above as discrete or continuous.\n\n\n\n\n\nInterpret each of the following in words and draw a picture representing it.\n\n\\(\\{R &gt; Y\\}\\).\n\n\n\n\n\n\\(\\{T &lt; 30\\}\\).\n\n\n\n\n\n\\(\\{W &lt; 15\\}\\).\n\n\n\n\n\n\\(\\{R &lt; 24\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Outcomes, Events, and Random Variables</span>"
    ]
  },
  {
    "objectID": "probability-models.html",
    "href": "probability-models.html",
    "title": "5  Probability Models",
    "section": "",
    "text": "A probability measure, typically denoted \\(\\text{P}\\), assigns probabilities to events to quantify their relative likelihoods according to the assumptions of the model of the random phenomenon.\nThe probability of event \\(A\\), computed according to probability measure \\(\\text{P}(A)\\), is denoted \\(\\text{P}(A)\\).\nA valid probability measure \\(\\text{P}\\) must satisfy the following three logical consistency “axioms”.\n\nFor any event \\(A\\), \\(0 \\le \\text{P}(A) \\le 1\\).\nIf \\(\\Omega\\) represents the sample space then \\(\\text{P}(\\Omega) = 1\\).\n(Countable additivity.) If \\(A_1, A_2, A_3, \\ldots\\) are disjoint then \\[\n\\text{P}(A_1 \\cup A_2 \\cup A_2 \\cup \\cdots) = \\text{P}(A_1) + \\text{P}(A_2) +\\text{P}(A_3) + \\cdots\n\\]\n\nAdditional properties of a probability measure follow from the axioms\n\nComplement rule. For any event \\(A\\), \\(\\text{P}(A^c)  = 1 - \\text{P}(A)\\).\nSubset rule. If \\(A \\subseteq B\\) then \\(\\text{P}(A) \\le \\text{P}(B)\\).\nAddition rule for two events. If \\(A\\) and \\(B\\) are any two events \\[\n\\text{P}(A\\cup B) = \\text{P}(A) + \\text{P}(B) - \\text{P}(A \\cap B)\n\\]\nLaw of total probability. If \\(C_1, C_2, C_3\\ldots\\) are disjoint events with \\(C_1\\cup C_2 \\cup C_3\\cup \\cdots =\\Omega\\), then \\[\n\\text{P}(A)  = \\text{P}(A \\cap C_1) + \\text{P}(A \\cap C_2) + \\text{P}(A \\cap C_3) + \\cdots\n\\]\n\nA probability model (or probability space) is the collection of all outcomes, events, and random variables associated with a random phenomenon along with the probabilities of all events of interest under the assumptions of the model.\nThe axioms of a probability measure are minimal logical consistent requirements that ensure that probabilities of different events fit together in a valid, coherent way.\nA single probability measure corresponds to a particular set of assumptions about the random phenomenon.\nThere can be many probability measures defined on a single sample space, each one corresponding to a different probability model for the random phenomenon.\nProbabilities of events can change if the probability measure changes.\n\n\nExample 5.1 Consider a single roll of a four-sided die. (Careful: don’t confuse these examples with other examples that involve two rolls.) The sample space is \\(\\{1, 2, 3, 4\\}\\). Table 5.1 lists all possible events.\n\nAdd a description in words for each of the events\n\n\n\n\n\nSuppose the die is fair, and let \\(\\text{P}\\) denote the correponding probability measure. Compute \\(\\text{P}(A)\\) for each event in Table 5.1.\n\n\n\n\n\n\n\n\n\n\nTable 5.1: All possible events associated with a single roll of a four-sided die.\n\n\n\n\n\n\n\n\n\n\n\n\n\\(A\\)\nDescription\n\\(\\text{P}(A)\\)\n\\(\\text{Q}(A)\\)\n\\(\\tilde{\\text{Q}}(A)\\)?\n\n\n\n\n\\(\\emptyset\\)\n      \n\n\n\n\n\n\\(\\{1\\}\\)\n\n\n\n\n\n\n\\(\\{2\\}\\)\n\n\n\n\n\n\n\\(\\{3\\}\\)\n\n\n\n\n\n\n\\(\\{4\\}\\)\n\n\n\n\n\n\n\\(\\{1, 2\\}\\)\n\n\n\n\n\n\n\\(\\{1, 3\\}\\)\n\n\n\n\n\n\n\\(\\{1, 4\\}\\)\n\n\n\n\n\n\n\\(\\{2, 3\\}\\)\n\n\n0.5\n\n\n\n\\(\\{2, 4\\}\\)\n\n\n\n\n\n\n\\(\\{3, 4\\}\\)\n\n\n0.7\n\n\n\n\\(\\{1, 2, 3\\}\\)\n\n\n0.6\n\n\n\n\\(\\{1, 2, 4\\}\\)\n\n\n\n\n\n\n\\(\\{1, 3, 4\\}\\)\n\n\n\n\n\n\n\\(\\{2, 3, 4\\}\\)\n\n\n\n\n\n\n\\(\\{1, 2, 3, 4\\}\\)\n\n\n\n\n\n\n\n\n\n\n\nExample 5.2 Consider a single roll of a four-sided die, but suppose the die is weighted so that the outcomes are no longer equally likely. Let \\(\\text{Q}\\) denote the probability measure corresponding to a particular weighting. A few probabilities are provided in Table 5.1; compute \\(\\text{Q}(A)\\) for all other events in Table 5.1. In what particular way is the die weighted? That is, what is the probability of each the four possible outcomes?\n\n\n\n\n\n\n\nExample 5.3 Consider a single roll of a different weighted four-sided die. Suppose that\n\nRolling a 1 is twice as likely as rolling a 4\nRolling a 2 is three times as likely as rolling a 4\nRolling a 3 is 1.5 times as likely as rolling a 4\n\nLet \\(\\tilde{\\text{Q}}\\) denote the probability measure corresponding to this particular weighting. Compute \\(\\tilde{\\text{Q}}(A)\\) for all events in Table 5.1.\nIn what particular way is the die weighted? That is, what is the probability of each the four possible outcomes?\n\n\n\n\n\n\n\nExample 5.4 The general meeting problem involves multiple people, but we’ll first consider the arrival time of just a single person, who we’ll call Han. Suppose that Han arrives “uniformly at random” at a time in \\([0, 60]\\).\n\nCompute the probability that Han arrives before 12:15.\n\n\n\n\n\nCompute the probability that Han arrives after 12:45.\n\n\n\n\n\nLet \\(\\text{P}\\) denote the corresponding probability measure. Suggest a general formula for \\(\\text{P}([a, b])\\), the probability that Han arrives between \\(a\\) and \\(b\\) minutes after noon for \\(0\\le a&lt;b\\le 60\\).\n\n\n\n\n\nCompute the probability that Han’s arrival time, truncated to the nearest minute, is 0 minutes after noon; that is, find the probability that Han arrives between 12:00 and 12:01.\n\n\n\n\n\nContinue to compute the probability that Han’s arrival time truncated to the nearest minute is \\(1, 2, 3, \\ldots, 59\\), and sketch a plot with arrival time (truncated minutes after noon) on the horizontal axis and probability on the vertical axis. Is the plot what you would expect for arriving “uniformly at random”?\n\n\n\n\n\n\n\n\nExample 5.5 Assume that the probability that Han arrives between \\(a\\) and \\(b\\) minutes after noon is \\((b/60)^2 - (a/60)^2\\), for \\(0\\le a&lt;b\\le 60\\). Let \\(\\text{Q}\\) denote the corresponding probability measure; notice that \\(\\text{Q}([0, 60]) = (60/60)^2 - (0/60)^2 = 1\\). Compute the following probabilities and compare your answers to the corresponding parts from Example 5.4.\n\nCompute the probability that Han arrives before 12:15.\n\n\n\n\n\nCompute the probability that Han arrives after 12:45.\n\n\n\n\n\nCompute the probability Han’s arrival time, truncated to the nearest minute, is 0 minutes after noon; that is, find the probability that Han arrives between 12:00 and 12:01.\n\n\n\n\n\nCompute the probability Han’s arrival time, truncated to the nearest minute, is 59 minutes after noon; that is, find the probability that Han arrives between 12:59 and 1:00.\n\n\n\n\n\nContinue to compute the probability that Han’s arrival time truncated to the nearest minute is \\(1, 2, 3, \\ldots, 59\\), and sketch a plot with arrival time (truncated minutes after noon) on the horizontal axis and probability on the vertical axis. What assumptions about Han’s arrival time does this probability measure reflect?\n\n\n\n\n\n\n\n\nExample 5.6 Continuing with the uniform probability measure of Example 5.4.\n\nCompute the probability that Han arrives between 12:00 and 12:01, within 1 minute after noon.\n\n\n\n\n\nCompute the probability that Han arrives between 12:00:00 and 12:00:01, within 1 second after noon.\n\n\n\n\n\nCompute the probability that Han arrives between 12:00:00.000 and 12:01:00.001, within 1 millisecond after noon.\n\n\n\n\n\nCompute the probability that Han arrives at the exact time 12:00:00.00000… (with infinite precision).\n\n\n\n\n\nWhat is the probability that Han arrives “at noon”? Discuss.\n\n\n\n\n\n\n\n\nExample 5.7 Continuing with the non-uniform probability measure of Example 5.5.\n\nCompute the probability that Han arrives between 12:00 and 12:01, within 1 minute after noon.\n\n\n\n\n\nCompute the probability that Han arrives between 12:00:00 and 12:00:01, within 1 second after noon.\n\n\n\n\n\nCompute the probability that Han arrives between 12:00:00.000 and 12:01:00.001, within 1 millisecond after noon.\n\n\n\n\n\nCompute the probability that Han arrives at the exact time 12:00:00.00000… (with infinite precision).\n\n\n\n\n\nCompute the probability that Han arrives between 12:59 and 1:00, within 1 minute before 1:00.\n\n\n\n\n\nCompute the probability that Han arrives between 12:59:59 and 1:00:00, within 1 second before 1:00.\n\n\n\n\n\nCompute the probability that Han arrives between 12:59:59.999 and 1:00:00.000, within 1 millisecond before 1:00.\n\n\n\n\n\nCompute the probability that Han arrives at the exact time 1:00:00.00000… (with infinite precision).\n\n\n\n\n\nWhich is more likely: that Han arrives “at noon” or “at 1:00”? Discuss.\n\n\n\n\n\n\n\n\nFor a continuous sample space, the probability of any particular outcome is 0.\nParticular outcomes represent “infinite precision” which is not practical in real applications\nFor continuous sample spaces it makes more sense to consider “close to” probabilities rather than “equals to” probabilities.\n“Close to” events correspond to intervals of reasonable practical precision and these intervals can have non-zero probability.\nCertain outcomes can be more likely than others in the “close to” sense.\n\n\nExample 5.8 Back to the Regina, Cady meeting problem. Assume that Regina and Cady each arrive at a time uniformly at random between noon and 1:00, independently of each other, so that they arrive “uniformly at random” in the sample space of Example 4.3. Let \\(\\text{P}\\) denote the corresponding probability measure.\nLet \\(R\\) be the random variable representing Regina’s arrival time (minutes after noon), and \\(Y\\) for Cady, and let \\(T = \\min(R, Y)\\) and \\(W = |R - Y|\\).\nCompute and interpret the following.\n\n\\(\\text{P}(R &gt; Y)\\)\n\n\n\n\n\n\\(\\text{P}(T &lt; 30)\\)\n\n\n\n\n\n\\(\\text{P}(W &lt; 15)\\)\n\n\n\n\n\n\\(\\text{P}(R &lt; 24)\\)\n\n\n\n\n\n\\(\\text{P}(W &lt; 1)\\)\n\n\n\n\n\n\\(\\text{P}(W = 0)\\)\n\n\n\n\n\nWhat is the probability that Regina and Cady arrive “at the same time”? Discuss.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability Models</span>"
    ]
  },
  {
    "objectID": "intro-to-distributions.html",
    "href": "intro-to-distributions.html",
    "title": "6  Distributions of Random Variables (A Brief Introduction)",
    "section": "",
    "text": "The joint (probability) distribution of a collection of random variables identifies the possible values that the random variables can take and their relative likelihoods.\nWe will see many ways of describing a distribution, depending on how many random variables are involved and their types (discrete or continuous).\nIn the context of multiple random variables, the distribution of any one of the random variables is called a marginal distribution.\n\n\nExample 6.1 Roll a fair four-sided die twice. Let \\(X\\) be the sum of the two dice, and let \\(Y\\) be the larger of the two rolls (or the common value if both rolls are the same).\n\n\nConstruct a table and plot displaying the marginal distribution of \\(Y\\).\n\n\n\n\n\nDescribe the distribution of \\(Y\\) in terms of long run relative frequency.\n\n\n\n\n\nDescribe the distribution of \\(Y\\) in terms of relative degree of likelihood.\n\n\n\n\n\nConstruct a table and plot displaying the joint distribution of \\(X\\) and \\(Y\\).\n\n\n\n\n\nConstruct a table and plot displaying the marginal distribution of \\(X\\).\n\n\n\n\n\n\n\nThe expected value \\(\\text{E}(X)\\) of a discrete random variable \\(X\\) is defined by the probability-weighted average according to the underlying probability measure.\nThe expected value of a random variable can be interpreted as the long-run average value of the random variable\n\n\nExercise 6.1 Consider the matching problem with \\(n=4\\): objects labeled 1, 2, 3, 4, are placed at random in spots labeled 1, 2, 3, 4, with spot 1 the correct spot for object 1, etc. Let the random variable \\(X\\) count the number of objects that are put back in the correct spot. Let \\(\\text{P}\\) denote the probability measure corresponding to the assumption that the objects are equally likely to be placed in any spot, so that the 24 possible placements are equally.\n\n\nFind the distribution of \\(X\\) by creating an appropriate table and plot.\n\n\n\n\n\nCompute \\(\\text{E}(X)\\).\n\n\n\n\n\nIs the value from part 2 the most likely value of \\(X\\)? Explain.\n\n\n\n\n\nIs the value from part 2 the value that we would “expect” to see for \\(X\\) in a single repetition of the phenomenon? Explain.\n\n\n\n\n\nExplain in what sense the value from part 2 is “expected”.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Distributions of Random Variables (A Brief Introduction)</span>"
    ]
  },
  {
    "objectID": "conditioning.html#footnotes",
    "href": "conditioning.html#footnotes",
    "title": "7  Conditioning",
    "section": "",
    "text": "Probabilities are estimated based on this 2024 survey.↩︎\nEstimate based on Gallup poll↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Conditioning</span>"
    ]
  },
  {
    "objectID": "probability-rules.html",
    "href": "probability-rules.html",
    "title": "8  Probability Rules",
    "section": "",
    "text": "8.1 Multiplication rule",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "probability-rules.html#multiplication-rule",
    "href": "probability-rules.html#multiplication-rule",
    "title": "8  Probability Rules",
    "section": "",
    "text": "Rearranging the definition of conditional probability we get the Multiplication rule: the probability that two events both occur is \\[\n\\begin{aligned}\n\\text{P}(A \\cap B) & = \\text{P}(A|B)\\text{P}(B)\\\\\n& = \\text{P}(B|A)\\text{P}(A)\n\\end{aligned}\n\\]\nThe multiplication rule says that you should think “multiply” when you see “and”.\nHowever, be careful about what you are multiplying: to find a joint probability you need an unconditional and an appropriate conditional probability.\nYou can condition either on \\(A\\) or on \\(B\\), provided you have the appropriate marginal probability; often, conditioning one way is easier than the other.\nBe careful: the multiplication rule does not say that \\(\\text{P}(A\\cap B)\\) is the same as \\(\\text{P}(A)\\text{P}(B)\\).\n\nThe multiplication rule is useful in situations where conditional probabilities are easier to obtain directly than joint probabilities.\n\n\nExample 8.1 A standard deck of playing cards has 52 cards, 13 cards (2 through 10, jack, king, queen, ace) in each of 4 suits (hearts, diamonds, clubs, spades). Shuffle a deck and deals cards one at a time without replacement.\n\n\nFind the probability that the first card dealt is a heart.\n\n\n\n\n\nIf the first card dealt is a heart, determine the conditional probability that the second card is a heart.\n\n\n\n\n\nFind the probability that the first two cards dealt are hearts.\n\n\n\n\n\nFind the probability that the first two cards dealt are hearts and the third card dealt is a diamond.\n\n\n\n\n\n\n\nThe multiplication rule extends naturally to more than two events (though the notation gets messy). For three events, we have \\[\n\\text{P}(A_1 \\cap A_2 \\cap A_3) = \\text{P}(A_1)\\text{P}(A_2|A_1)\\text{P}(A_3|A_1\\cap A_2)\n\\]\nAnd in general, \\[\n\\text{P}(A_1\\cap A_2 \\cap A_3 \\cap A_4 \\cap \\cdots) = \\text{P}(A_1)\\text{P}(A_2|A_1)\\text{P}(A_3|A_1\\cap A_2)\\text{P}(A_4|A_1\\cap A_2 \\cap A_4)\\cdots\n\\]\nThe multiplication rule is useful for computing probabilities of events that can be broken down into component “stages” where conditional probabilities at each stage are readily available. At each stage, condition on the information about all previous stages.\n\n\nExample 8.2 The birthday problem concerns the probability that at least two people in a group of \\(n\\) people have the same birthday1. Ignore multiple births and February 29 and assume that the other 365 days are all equally likely2.\n\n\nIf \\(n=30\\), what do you think the probability that at least two people share a birthday is: 0-20%, 20-40%, 40-60%, 60-80%, 80-100%? How large do you think \\(n\\) needs to be in order for the probability that at least two people share a birthday to be larger than 0.5? Just make guesses before proceeding to calculations.\n\n\n\n\n\nNow consider \\(n=3\\) people, labeled 1, 2, and 3. What is the probability that persons 1 and 2 have different birthdays?\n\n\n\n\n\nWhat is the probability that persons 1, 2, and 3 all have different birthdays given that persons 1 and 2 have different birthdays?\n\n\n\n\n\nWhat is the probability that persons 1, 2, and 3 all have different birthdays?\n\n\n\n\n\nWhen \\(n = 3\\). What is the probability that at least two people share a birthday?\n\n\n\n\n\nFor \\(n=30\\), find the probability that none of the people have the same birthday.\n\n\n\n\n\nFor \\(n=30\\), find the probability that at least two people have the same birthday.\n\n\n\n\n\nWrite a clearly worded sentence interpreting the probability in the previous part as a long run relative frequency.\n\n\n\n\n\nWhen \\(n=100\\) the probability is about 0.9999997.\nIf you are in a group of 100 people and no one shares your birthday, should you be surprised? Discuss.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "probability-rules.html#law-of-total-probability",
    "href": "probability-rules.html#law-of-total-probability",
    "title": "8  Probability Rules",
    "section": "8.2 Law of total probability",
    "text": "8.2 Law of total probability\n\nExample 8.3 Suppose that\n\n67% of Democrats believe in human-driven climate\n46% of Independents believe in human-driven climate\n34% of Republicans believe in human-driven climate\n\nAlso suppose that\n\n28% of American adults are Democrats\n42% of American adults are Independents\n30% of American adults are Republicans\n\nRandomly select an American adult. Compute the probability that the selected person believes in human-driven climate change.\n             \n\n\nLaw of total probability. If \\(C_1,\\ldots, C_k\\) are disjoint with \\(C_1\\cup \\cdots \\cup C_k=\\Omega\\), then \\[\\begin{align*}\n\\text{P}(A) & = \\sum_{i=1}^k \\text{P}(A \\cap C_i)\\\\\n& = \\sum_{i=1}^k \\text{P}(A|C_i) \\text{P}(C_i)\n\\end{align*}\\]\nThe events \\(C_1, \\ldots, C_k\\), which represent the “cases”, form a partition of the sample space; each outcome \\(\\omega\\in\\Omega\\) lies in exactly one of the \\(C_i\\).\nThe law of total probability says that we can interpret the unconditional probability \\(\\text{P}(A)\\) as a probability-weighted average of the case-by-case conditional probabilities \\(\\text{P}(A|C_i)\\) where the weights \\(\\text{P}(C_i)\\) represent the probability of encountering each case.\nConditioning and using the law of probability is an effective strategy in solving many problems, even when the problem doesn’t seem to involve conditioning.\nFor example, when a problem involves iterations or steps it is often useful to condition on the result of the first step.\n\n\nExample 8.4 You and your friend are playing the “lookaway challenge”.\nThe game consists of possibly multiple rounds. In the first round, you point in one of four directions: up, down, left or right. At the exact same time, your friend also looks in one of those four directions. If your friend looks in the same direction you’re pointing, you win! Otherwise, you switch roles and the game continues to the next round — now your friend points in a direction and you try to look away. As long as no one wins, you keep switching off who points and who looks. The game ends, and the current “pointer” wins, whenever the “looker” looks in the same direction as the pointer.\nSuppose that each player is equally likely to point/look in each of the four directions, independently from round to round. What is the probability that you win the game?\n\n\nWhy might you expect the probability to not be equal to 0.5?\n\n\n\n\n\nIf you start as the pointer, what is the probability that you win in the first round?\n\n\n\n\n\nIf \\(p\\) denotes the probability that the player who starts as the pointer wins the game, what is the probability that the player who starts as the looker wins the game? (Note: \\(p\\) is the probability that the person who starts as pointer wins the whole game, not just the first round.)\n\n\n\n\n\nLet \\(A\\) be the event that the person who starts as the pointer wins the game, and \\(B\\) be the event that the person who starts as the pointer wins in the first round. What is \\(\\text{P}(A|B)\\)?\n\n\n\n\n\nFind a simple expression for \\(\\text{P}(A | B^c)\\) in terms of \\(p\\). The key is to consider this question: if the player who starts as the pointer does not win in the first round, how does the game behave from that point forward?\n\n\n\n\n\nCondition on the result of the first round and set up an equation to solve for \\(p\\).\n\n\n\n\n\nInterpret the probability from the previous part.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "probability-rules.html#bayes-rule",
    "href": "probability-rules.html#bayes-rule",
    "title": "8  Probability Rules",
    "section": "8.3 Bayes Rule",
    "text": "8.3 Bayes Rule\n\nExample 8.5 Suppose that\n\n67% of Democrats believe in human-driven climate\n46% of Independents believe in human-driven climate\n34% of Republicans believe in human-driven climate\n\nAlso suppose that\n\n28% of American adults are Democrats\n42% of American adults are Independents\n30% of American adults are Republicans\n\nRandomly select an American adult. Compute the conditional probability that the selected adult is a Democrat given that they believe in human-driven climate change.\n\n\n\n\n\n\n\nBayes’ rule for events specifies how a prior probability \\(P(H)\\) of event \\(H\\) is updated in response to the evidence \\(E\\) to obtain the posterior probability \\(P(H|E)\\). \\[\n\\text{P}(H|E) = \\frac{\\text{P}(E|H)\\text{P}(H)}{\\text{P}(E)}\n\\]\nEvent \\(H\\) represents a particular hypothesis (or model or case)\nEvent \\(E\\) represents observed evidence (or data or information)\n\\(\\text{P}(H)\\) is the unconditional or prior probability of \\(H\\) (prior to observing evidence \\(E\\))\n\\(\\text{P}(H|E)\\) is the conditional or posterior probability of \\(H\\) after observing evidence \\(E\\).\n\\(\\text{P}(E|H)\\) is the likelihood of evidence \\(E\\) given hypothesis (or model or case) \\(H\\)\n\n\nExample 8.6 Continuing the previous example. Randomly select an American adult.\n\n\nConsider the conditional probability that the selected adult is a Democrat given that they believe in human-driven climate change. Identify the prior probability, hypothesis, evidence, likelihood, and posterior probability.\n\n\n\n\n\nCompute the conditional probability that the selected adult is a Republican given that they believe in human-driven climate change.\n\n\n\n\n\nHow many times more likely is it for an American adult to be a Democrat than be be a Republican?\n\n\n\n\n\nHow many times more likely is it for an American adult to believe in human-driven climate change given that they are a Democrat than given that they are a Republican?\n\n\n\n\n\nHow many times more likely is it for an American adult who believes in human-driven climate change to be a Democrat than to be a Republican?\n\n\n\n\n\nHow are the values in the three previous parts related?\n\n\n\n\n\nCompute the conditional probability that the selected adult is an Independent given that they believe in human-driven climate change.\n\n\n\n\n\nHow many times more likely is it for an American adult to be an Independent than be be a Republican?\n\n\n\n\n\nHow many times more likely is it for an American adult to believe in human-driven climate change given that they are an Independent than given that they are a Republican?\n\n\n\n\n\nHow many times more likely is it for an American adult who believes in human-driven climate change to be an Independent than to be a Republican?\n\n\n\n\n\nHow are the values in the three previous parts related?\n\n\n\n\n\n\n\nBayes rule is often used when there are multiple hypotheses or cases. Suppose \\(H_1,\\ldots, H_k\\) is a series of distinct hypotheses which together account for all possibilities, and \\(E\\) is any event (evidence).\nCombining Bayes’ rule with the law of total probability, \\[\\begin{align*}\nP(H_j |E) & = \\frac{P(E|H_j)P(H_j)}{P(E)}\\\\\n& = \\frac{P(E|H_j)P(H_j)}{\\sum_{i=1}^k P(E|H_i) P(H_i)}\\\\\n& \\\\\nP(H_j |E) & \\propto P(E|H_j)P(H_j)\n\\end{align*}\\]\nThe symbol \\(\\propto\\) is read “is proportional to”. The relative ratios of the posterior probabilities of different hypotheses are determined by the product of the prior probabilities and the likelihoods, \\(P(E|H_j)P(H_j)\\). The marginal probability of the evidence, \\(P(E)\\), in the denominator simply normalizes the numerators to ensure that the updated conditional probabilities given the evidence sum to 1 over all the distinct hypotheses.\nIn short, Bayes’ rule says \\[\n\\textbf{posterior} \\propto \\textbf{likelihood} \\times \\textbf{prior}\n\\]\n\n\nExample 8.7 Suppose that you are presented with six boxes, labeled 0, 1, 2, \\(\\ldots\\), 5, each containing five marbles. Box 0 contains 0 green and 5 gold marbles, box 1 contains 1 green and 4 gold, and so on with box \\(i\\) containing \\(i\\) green and \\(5-i\\) gold. One of the boxes is chosen uniformly at random (perhaps by rolling a fair six-sided die), and then you will randomly select marbles from that box, without replacement. Imagine the boxes appear identical and you can’t see inside; all you observe is the color of the marbles you select. Based on the colors of the marbles selected, you will update the probabilities of which box had been chosen.\n\nSuppose that a single marble is selected and it is green. Which box do you think is the most likely to have been chosen? Make a guess for the posterior probabilities for each box. Then construct a Bayes table to compute the posterior probabilities. How do they compare to the prior probabilities?\n\n\n\n\n\nNow suppose a second marble is selected from the same box, without replacement, and its color is gold. Which box do you think is the most likely to have been chosen given these two marbles? Make a guess for the posterior probabilities for each box. Then construct a Bayes table to compute the posterior probabilities, using the posterior probabilities from the previous part after the selection of the green marble as the new prior probabilities before seeing the gold marble.\n\n\n\n\n\nNow construct a Bayes table corresponding to the original prior probabilities (1/6 each) and the combined evidence that the first ball selected was green and the second was gold. How do the posterior probabilities compare to the previous part?\n\n\n\n\n\n\n\n\nBayesian analysis is often an iterative process.\nPosterior probabilities are updated after observing some information or data. These probabilities can then be used as prior probabilities before observing new data.\nPosterior probabilities can be sequentially updated as new data becomes available, with the posterior probabilities after the previous stage serving as the prior probabilities for the next stage.\nThe final posterior probabilities only depend upon the cumulative data. It doesn’t matter if we sequentially update the posterior after each new piece of data or only once after all the data is available; the final posterior probabilities will be the same either way. Also, the final posterior probabilities are not impacted by the order in which the data are observed.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "probability-rules.html#footnotes",
    "href": "probability-rules.html#footnotes",
    "title": "8  Probability Rules",
    "section": "",
    "text": "You should really click on this birthday problem link.↩︎\nWhich isn’t quite true. However, a non-uniform distribution of birthdays only increases the probability that at least two people have the same birthday. To see that, think of an extreme case like if everyone were born in September.↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Probability Rules</span>"
    ]
  },
  {
    "objectID": "independence.html",
    "href": "independence.html",
    "title": "9  Independence",
    "section": "",
    "text": "Example 9.1 Consider the following hypothetical data.\n\n\n\n\n\n\n\n\n\n\nDemocrat (\\(D\\))\nNot Democrat (\\(D^c\\))\nTotal\n\n\n\n\nLoves puppies (\\(L\\))\n180\n270\n450\n\n\nDoes not love puppies (\\(L^c\\))\n20\n30\n50\n\n\nTotal\n200\n300\n500\n\n\n\nSuppose a person is randomly selected from this group. Consider the events \\[\\begin{align*}\nL & = \\{\\text{person loves puppies}\\}\\\\\nD & = \\{\\text{person is a Democrat}\\}\n\\end{align*}\\]\n\n\nCompute and interpret \\(\\text{P}(L)\\).\n\n\n\n\n\nCompute and interpret \\(\\text{P}(L|D)\\).\n\n\n\n\n\nCompute and interpret \\(\\text{P}(L|D^c)\\).\n\n\n\n\n\nWhat do you notice about \\(\\text{P}(L)\\), \\(\\text{P}(L|D)\\), and \\(\\text{P}(L|D^c)\\)?\n\n\n\n\n\nCompute and interpret \\(\\text{P}(D)\\).\n\n\n\n\n\nCompute and interpret \\(\\text{P}(D|L)\\).\n\n\n\n\n\nCompute and interpret \\(\\text{P}(D|L^c)\\).\n\n\n\n\n\nWhat do you notice about \\(\\text{P}(D)\\), \\(\\text{P}(D|L)\\), and \\(\\text{P}(D|L^c)\\)?\n\n\n\n\n\nCompute and interpret \\(\\text{P}(D \\cap L)\\).\n\n\n\n\n\nWhat is the relationship between \\(\\text{P}(D \\cap L\\)) and \\(\\text{P}(D)\\) and \\(\\text{P}(L)\\)?\n\n\n\n\n\nWhen randomly selecting a person from this particular group, would you say that events \\(D\\) and \\(L\\) are independent? Why?\n\n\n\n\n\n\n\nEvents \\(A\\) and \\(B\\) are independent if the knowing whether or not one occurs does not change the probability of the other.\nFor events \\(A\\) and \\(B\\) (with \\(0&lt;\\text{P}(A)&lt;1\\) and \\(0&lt;\\text{P}(B)&lt;1\\)) the following are equivalent. That is, if one is true then they all are true; if one is false, then they all are false.\n\n\\[\\begin{align*}\n\\text{$A$ and $B$} & \\text{ are independent}\\\\\n\\text{P}(A \\cap B) & = \\text{P}(A)\\text{P}(B)\\\\\n\\text{P}(A^c \\cap B) & = \\text{P}(A^c)\\text{P}(B)\\\\\n\\text{P}(A \\cap B^c) & = \\text{P}(A)\\text{P}(B^c)\\\\\n\\text{P}(A^c \\cap B^c) & = \\text{P}(A^c)\\text{P}(B^c)\\\\\n\\text{P}(A|B) & = \\text{P}(A)\\\\\n\\text{P}(A|B) & = \\text{P}(A|B^c)\\\\\n\\text{P}(B|A) & = \\text{P}(B)\\\\\n\\text{P}(B|A) & = \\text{P}(B|A^c)\n\\end{align*}\\]\n\nExample 9.2 Each of the three Venn diagrams below represents a sample space with 16 equally likely outcomes. Let \\(A\\) be the yellow / event, \\(B\\) the blue \\ event, and their intersection \\(A\\cap B\\) the green \\(\\times\\) event. Suppose that areas represent probabilities, so that for example \\(\\text{P}(A) = 4/16\\).\nIn which of the scenarios are events \\(A\\) and \\(B\\) independent?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo not confuse “disjoint” with “independent”.\nDisjoint means two events do not “overlap”. Independence means two events “overlap in just the right way”.\nYou can pretty much forget “disjoint” exists; you will naturally apply the addition rule for disjoint events correctly without even thinking about it.\nIndependence is much more important and useful, but also requires more care.\n\n\nExample 9.3 Roll two fair six-sided dice, one green and one gold. There are 36 total possible outcomes (roll on green, roll on gold), all equally likely. Consider the event \\(E=\\{\\text{the green die lands on 1}\\}\\). Answer the following questions by computing and comparing appropriate probabilities.\n\n\nConsider \\(A=\\{\\text{the gold die lands on 6}\\}\\). Are \\(A\\) and \\(E\\) independent?\n\n\n\n\n\nConsider \\(B=\\{\\text{the sum of the dice is 2}\\}\\). Are \\(B\\) and \\(E\\) independent?\n\n\n\n\n\nConsider \\(C=\\{\\text{the sum of the dice is 7}\\}\\). Are \\(C\\) and \\(E\\) independent?\n\n\n\n\n\n\n\nIndependence concerns whether or not the occurrence of one event affects the probability of the other.\nGiven two events it is not always obvious whether or not they are independent.\n\nIndependence depends on the underlying probability measure. Events that are independent under one probability measure might not be independent under another.\nIndependence is often assumed. Whether or not independence is a valid assumption depends on the underlying random phenomenon.\n\n\nExample 9.4 You have just been elected president (congratulations!) and you need to choose one of four people to sing the national anthem at your inauguration: Alicia, Ariana, Beyonce, or Billie. You write their names on some cards — each name on possibly a different number of cards — shuffle the cards, and draw one. Let \\(A\\) be the event that either Alicia or Ariana is selected, and \\(B\\) be the event that either Alicia or Beyonce is selected.\nThe following questions ask you to specify probability models satisfying different conditions. You can specify the model by identifying how many cards each person’s name is written on. For each model, find the probabilities of \\(A\\), \\(B\\), and \\(A\\cap B\\), and verify whether or not events \\(A\\) and \\(B\\) are independent according to the model.\n\n\nSpecify a probability model according to which the events \\(A\\) and \\(B\\) are independent.\n\n\n\n\n\nSpecify a different probability model according to which the events \\(A\\) and \\(B\\) are independent.\n\n\n\n\n\nSpecify a probability model according to which the events \\(A\\) and \\(B\\) are not independent.\n\n\n\n\n\n\n\nExample 9.5 Flip a fair coin twice. Let\n\n\\(A\\) be the event that the first flip lands on heads\n\\(B\\) be the event that the second flip lands on heads,\n\\(C\\) be the event that both flips land on the same side.\n\n\n\nAre the two events \\(A\\) and \\(B\\) independent?\n\n\n\n\n\n\nAre the two events \\(A\\) and \\(C\\) independent?\n\n\n\n\n\n\nAre the two events \\(B\\) and \\(C\\) independent?\n\n\n\n\n\n\nAre the three events \\(A\\), \\(B\\), and \\(C\\) independent?\n\n\n\n\n\n\n\n\nEvents \\(A_1, A_2, A_3, \\ldots\\) are independent if:\n\nany pair of events \\(A_i, A_j, (i \\neq j)\\) satisfies \\(\\text{P}(A_i\\cap A_j)=\\text{P}(A_i)\\text{P}(A_j)\\),\nand any triple of events \\(A_i, A_j, A_k\\) (distinct \\(i,j,k\\)) satisfies \\(\\text{P}(A_i\\cap A_j\\cap A_k)=\\text{P}(A_i)\\text{P}(A_j)\\text{P}(A_k)\\),\nand any quadruple of events satisfies \\(\\text{P}(A_i\\cap A_j\\cap A_k \\cap A_m)=\\text{P}(A_i)\\text{P}(A_j)\\text{P}(A_k)\\text{P}(A_m)\\),\nand so on.\n\nIntuitively, a collection of events is independent if knowing whether or not any combination of the events in the collection occur does not change the probability of any other event in the collection.\n\n\nExample 9.6 A certain system consists of four identical components. Suppose that the probability that any particular component fails is 0.1, and failures of the components occur independently of each other. Find the probability that the system fails if:\n\n\nThe components are connected in parallel: the system fails only if all of the components fail.\n\n\n\n\n\nThe components are connected in series: the system fails whenever at least one of the components fails.\n\n\n\n\n\nDonny Don’t says the answer to the previous part is \\(0.1 + 0.1 + 0.1 + 0.1 = 0.4\\). Explain the error in Donny’s reasoning.\n\n\n\n\n\n\n\nWhen events are independent, the multiplication rule simplifies greatly. \\[\n\\text{P}(A_1 \\cap A_2 \\cap A_3 \\cap \\cdots \\cap A_n) = \\text{P}(A_1)\\text{P}(A_2)\\text{P}(A_3)\\cdots\\text{P}(A_n) \\quad \\text{if $A_1, A_2, A_3, \\ldots, A_n$ are independent}\n\\]\nWhen a problem involves independence, you will want to take advantage of it. Work with “and” events whenever possible in order to use the multiplication rule.\nFor example, for problems involving “at least one” (an “or” event) take the complement to obtain “none” (an “and” event).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Independence</span>"
    ]
  }
]